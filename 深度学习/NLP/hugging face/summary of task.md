# summary of task

## 1.åºåˆ—åˆ†ç±»

æ­¥éª¤

1. å®ä¾‹åŒ–Tokenizerå’ŒxxxForSequenceClassificationï¼Œé€‰æ‹©æ¨¡å‹ã€‚
2. ä½¿ç”¨æ­£ç¡®çš„ç‰¹å®šäºæ¨¡å‹çš„åˆ†éš”ç¬¦æ ‡è®°ç±»å‹IDå’Œæ³¨æ„æ©ç 
3. å°†æ­¤åºåˆ—ä¼ é€’ç»™æ¨¡å‹ï¼Œä»¥ä¾¿å°†å…¶åˆ†ç±»
4. è®¡ç®—ç»“æœçš„softmaxä»¥è·å¾—å„ç±»çš„æ¦‚ç‡ã€‚
5. Print the results.

```
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
# å®ä¾‹åŒ–Tokenizerå’ŒForSequenceClassification  é€‰æ‹©bert-base-cased-finetuned-mrpcæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased-finetuned-mrpc")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased-finetuned-mrpc")

classes = ["not paraphrase", "is paraphrase"]
sequence_0 = "The company HuggingFace is based in New York City"
sequence_1 = "Apples are especially bad for your health"
sequence_2 = "HuggingFace's headquarters are situated in Manhattan"
# è¿›è¡Œtoken
paraphrase = tokenizer(sequence_0, sequence_2, return_tensors="pt")
not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors="pt")
# è¿›è¡Œåˆ†ç±»
paraphrase_classification_logits = model(**paraphrase).logits
not_paraphrase_classification_logits = model(**not_paraphrase).logits
# ç»Ÿè®¡å„ç±»æ¦‚ç‡
paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]
not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]

# Should be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")
# Should not be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")
```

## 2.æå–å¼é—®ç­”

æ­¥éª¤

1. å®ä¾‹åŒ–ï¼Œé€‰æ‹©æ¨¡å‹ã€‚
2. å®šä¹‰æ–‡æœ¬å’Œä¸€äº›é—®é¢˜
3. å°†é—®é¢˜å’Œæ–‡æœ¬ä¾æ¬¡è¿›è¡Œtokenï¼Œè¿›è¡Œè®­ç»ƒ
4. è·å–åšå¤§å¯èƒ½çš„å¼€å§‹å’Œç»“æŸçš„token_id
5. å°†token_idè¿˜åŸæˆå¥å­

```
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch
tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
model = AutoModelForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
text = r"""
ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural
Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
TensorFlow 2.0 and PyTorch.
"""
questions = [
    "How many pretrained models are available in ğŸ¤— Transformers?",
    "What does ğŸ¤— Transformers provide?",
    "ğŸ¤— Transformers provides interoperability between which frameworks?",
]
for question in questions:
    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors="pt")
    input_ids = inputs["input_ids"].tolist()[0]
    outputs = model(**inputs)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits
    answer_start = torch.argmax(
        answer_start_scores
    )  # Get the most likely beginning of answer with the argmax of the score
    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
    print(f"Question: {question}")
    print(f"Answer: {answer}")
```



## 3.è¯­è¨€æ¨¡å‹

è¯­è¨€å»ºæ¨¡æ˜¯ä½¿æ¨¡å‹é€‚åˆè¯­æ–™åº“çš„ä»»åŠ¡ï¼Œè¯­æ–™åº“å¯ä»¥æ˜¯ç‰¹å®šäºé¢†åŸŸçš„ã€‚

**Masked Language Modeling(BERT)**

å…ˆé®æ©éƒ¨åˆ†tokenï¼Œç„¶åè®­ç»ƒï¼Œé¢„æµ‹è¿™äº›tokenã€‚æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹ã€‚

æ­¥éª¤

1. å®ä¾‹åŒ–ï¼Œé€‰æ‹©æ¨¡å‹ã€‚
2. ç”¨**tokenizer.mask_token**æ›¿æ¢æ–‡ä¸­éœ€è¦è¢«é®æ©çš„è¯
3. å°†åºåˆ—è¿›è¡Œç¼–ç ï¼ˆencodeï¼‰æˆtoken_idï¼Œå¹¶ä¸”æ ‡è®°mask_attentationçš„ä½ç½®
4. å°†åºåˆ—çš„token_idè½¬æˆè¯æ±‡è¡¨ç›¸åŒå¤§å°çš„å¼ é‡ï¼Œç„¶åè®¡ç®—å…¶åˆ†æ•°ã€‚
5. å°†å‰å‡ ä¸ªåˆ†æ•°é«˜çš„è¯æ±‡å–å‡º
6. å°†token_idè¿˜åŸæˆå¥å­

```
from transformers import AutoModelWithLMHead, AutoTokenizer
import torch
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-cased")
model = AutoModelWithLMHead.from_pretrained("distilbert-base-cased")
sequence = f"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint."
input = tokenizer.encode(sequence, return_tensors="pt")
mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]
token_logits = model(input).logits
mask_token_logits = token_logits[0, mask_token_index, :]
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()
```

**Causal Language Modelingï¼ˆGPTï¼‰**

æ¨¡å‹ä»…å…³æ³¨å·¦ä¾§ä¸Šä¸‹æ–‡æ¨¡å‹ä»…å…³æ³¨å·¦ä¾§ä¸Šä¸‹æ–‡.é€šè¿‡ä»æ¨¡å‹ä»è¾“å…¥åºåˆ—äº§ç”Ÿçš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€çš„å¯¹æ•°ä¸­é‡‡æ ·ï¼Œå¯ä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚

```
from transformers import AutoModelWithLMHead, AutoTokenizer, top_k_top_p_filtering
import torch
from torch.nn import functional as F
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelWithLMHead.from_pretrained("gpt2")
sequence = f"Hugging Face is based in DUMBO, New York City, and"
input_ids = tokenizer.encode(sequence, return_tensors="pt")
# get logits of last hidden state
next_token_logits = model(input_ids).logits[:, -1, :]
# filter
filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)
# sample
probs = F.softmax(filtered_next_token_logits, dim=-1)
next_token = torch.multinomial(probs, num_samples=1)
generated = torch.cat([input_ids, next_token], dim=-1)
resulting_string = tokenizer.decode(generated.tolist()[0])
```

**Text Generation**

```
from transformers import AutoModelWithLMHead, AutoTokenizer
model = AutoModelWithLMHead.from_pretrained("xlnet-base-cased")
tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology
PADDING_TEXT = """In 1991, the remains of Russian Tsar Nicholas II and his family
(except for Alexei and Maria) are discovered.
The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the
remainder of the story. 1883 Western Siberia,
a young Grigori Rasputin is asked by his father and a group of men to perform magic.
Rasputin has a vision and denounces one of the men as a horse thief. Although his
father initially slaps him for making such an accusation, Rasputin watches as the
man is chased outside and beaten. Twenty years later, Rasputin sees a vision of
the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,
with people, even a bishop, begging for his blessing. <eod> </s> <eos>"""
prompt = "Today the weather is really nice and I am planning on "
inputs = tokenizer.encode(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors="pt")
prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))
outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)
generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]
```

## 4.Named Entity Recognition (NER) 

æ ¹æ®ç±»åˆ«å¯¹tokenè¿›è¡Œåˆ†ç±»çš„ä»»åŠ¡ï¼Œ a token as a person, an organisation or a location

å°†tokenåˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š

- O,å‘½åå®ä½“ä»¥å¤–
- B-MIS,ä¸€ä¸ªæ‚é¡¹å®ä½“çš„å¼€å§‹ï¼Œç´§æ¥ç€å¦ä¸€ä¸ªæ‚é¡¹å®ä½“
- I-MIS,æ‚é¡¹å®ä½“
- B-PER,ä¸€ä¸ªäººåå­—çš„å¼€å¤´ï¼Œç´§è·Ÿå¦ä¸€ä¸ªäººçš„åå­—
- I-PER,äººå
- B-OPG,ä¸€ä¸ªç»„ç»‡çš„æˆç«‹ï¼Œç´§æ¥ç€å¦ä¸€ä¸ªç»„ç»‡
- I-ORG,ç»„ç»‡
- B-LOC,ä¸€ä¸ªä½ç½®çš„èµ·ç‚¹ç´§æ¥å¦ä¸€ä¸ªä½ç½®
- I-LOC,åœ°ç‚¹

æ­¥éª¤

1. å®ä¾‹åŒ–ï¼Œé€‰æ‹©æ¨¡å‹ã€‚
2. å®šä¹‰ç”¨äºè®­ç»ƒæ¨¡å‹çš„æ ‡ç­¾åˆ—è¡¨ã€‚
3. ç”¨å·²çŸ¥å®ä½“å®šä¹‰åºåˆ—
4. ç¼–ç è¿›è¡Œé¢„æµ‹
5. è§£ç è¾“å‡º

```
from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch
model = AutoModelForTokenClassification.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
label_list = [
    "O",       # Outside of a named entity
    "B-MISC",  # Beginning of a miscellaneous entity right after another miscellaneous entity
    "I-MISC",  # Miscellaneous entity
    "B-PER",   # Beginning of a person's name right after another person's name
    "I-PER",   # Person's name
    "B-ORG",   # Beginning of an organisation right after another organisation
    "I-ORG",   # Organisation
    "B-LOC",   # Beginning of a location right after another location
    "I-LOC"    # Location
]
sequence = "Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very" \
           "close to the Manhattan Bridge."
# Bit of a hack to get the tokens with the special tokens
tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))
inputs = tokenizer.encode(sequence, return_tensors="pt")
outputs = model(inputs).logits
predictions = torch.argmax(outputs, dim=2)
```

## 5.æ‘˜è¦

1. å®ä¾‹åŒ–ï¼Œé€‰æ‹©æ¨¡å‹ã€‚
2. å®šä¹‰åº”æ€»ç»“çš„æ–‡ç« 
3. æ·»åŠ ç‰¹å®šäºT5çš„å‰ç¼€â€œ summaryizeï¼šâ€ã€‚
4. ä½¿ç”¨PreTrainedModel.generateï¼ˆï¼‰æ–¹æ³•ç”Ÿæˆæ‘˜è¦

```
from transformers import AutoModelWithLMHead, AutoTokenizer
model = AutoModelWithLMHead.from_pretrained("t5-base")
tokenizer = AutoTokenizer.from_pretrained("t5-base")
# T5 uses a max_length of 512 so we cut the article to 512 tokens.
inputs = tokenizer.encode("summarize: " + ARTICLE, return_tensors="pt", max_length=512)
outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
```

## 6.ç¿»è¯‘

1. å®ä¾‹åŒ–ï¼Œé€‰æ‹©æ¨¡å‹ã€‚
2. å®šä¹‰åº”æ€»ç»“çš„æ–‡ç« 
3. æ·»åŠ ç‰¹å®šäºT5çš„å‰ç¼€â€œtranslate English to German: â€œã€‚
4. ä½¿ç”¨PreTrainedModel.generateï¼ˆï¼‰æ–¹æ³•ç”Ÿæˆæ‘˜è¦

```
from transformers import AutoModelWithLMHead, AutoTokenizer
model = AutoModelWithLMHead.from_pretrained("t5-base")
tokenizer = AutoTokenizer.from_pretrained("t5-base")
inputs = tokenizer.encode("translate English to German: Hugging Face is a technology company based in New York and Paris", return_tensors="pt")
outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)
```


# å…¥é—¨æ•™ç¨‹

## ä¸€ã€èƒ½åšä»€ä¹ˆ

1.è¯¥æ¡†æ¶åŠŸèƒ½ï¼šæƒ…æ„Ÿåˆ†æã€æ–‡æœ¬ç”Ÿæˆï¼ˆè‹±è¯­ï¼‰ã€å‘½åå®ä½“ï¼ˆNERï¼‰ã€é—®ç­”ã€å¡«ç©ºã€æ€»ç»“æ‘˜è¦ã€ç¿»è¯‘ã€ç‰¹å¾æå–ã€‚

ä¾‹å¦‚ï¼š

```python
# å¼•å…¥åº“ï¼Œè®¾ç½®ç®¡é“
from transformers import pipeline
# ç®¡é“ä»»åŠ¡åç§°ï¼Œä½¿ç”¨é»˜è®¤çš„åˆ†ç±»æ¨¡å‹
classifier = pipeline('sentiment-analysis')

# è¿›è¡Œé¢„æµ‹
classifier('We are very happy to show you the ğŸ¤— Transformers library.')

# é€‰æ‹©å…¶ä»–çš„æ¨¡å‹ å¯ä»¥ä»model huï¼ˆhttps://huggingface.co/modelsï¼‰ä¸Šé¢é€‰æ‹© ä¾‹å¦‚é€‰æ‹©nlptown/bert-base-multilingual-uncased-sentimentã€‚ä¹Ÿå¯ä»¥ç”¨æœ¬åœ°æ–‡ä»¶ä»£æ›¿
classifier = pipeline('sentiment-analysis', model="nlptown/bert-base-multilingual-uncased-sentiment")
```

```
# éœ€è¦å®ä¾‹åŒ–ä¸¤ä¸ªå¯¹è±¡ï¼ŒAutoTokenizerï¼šæ˜¯å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œåˆ†è¯æ¨¡å‹å’Œåˆ†ç±»æ¨¡å‹éœ€è¦ä¸€ä¸€å¯¹åº”ã€‚
# AutoModelForSequenceClassificationï¼šç”¨äºæ–‡æœ¬åˆ†ç±»
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# é€‰æ‹©æ¨¡å‹
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
# ä¸‹è½½å¹¶å®ä¾‹åˆ†ç±»æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(model_name)
# ä¸‹è½½å¹¶å®ä¾‹åˆ†è¯æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_name)
# è¾“å…¥åˆ°ç®¡é“å†…
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
```

## 2.åˆ†æé¢„è®­ç»ƒæ¨¡å‹

```
# åŒä¸Š
from transformers import AutoTokenizer, AutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

åˆ†è¯å™¨ï¼ˆtokenizerï¼‰æ˜¯å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå¯¹æ–‡æœ¬è¿›è¡Œåˆ‡åˆ†ï¼ˆè¯ã€æ ‡ç‚¹ã€ç¬¦å·ï¼‰ç§°ä¸ºåˆ†è¯ã€‚

```
inputs = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
inputs
# {'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
# è¿”å›çš„æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œinput_idsï¼šæ¯ä¸ªå•è¯åœ¨å­—å…¸ä¸­çš„idï¼Œ attention_maskï¼šè¡¨ç¤ºæ˜¯å¦MASKLE 1 è¡¨ç¤ºæ²¡æœ‰
```















## 4.åŸºæœ¬æ¦‚å¿µ

è¯¥æ¡†æ¶ä»…ä»…éœ€è¦å®ä¾‹åŒ–ä¸‰ä¸ªç±»ï¼Œå…¶ä»–å¯¹è±¡éƒ½è¢«å°è£…ã€‚è¿™äº›ç±»é€šè¿‡ç»Ÿä¸€çš„from_pretrained()æ–¹æ³•å®ç°ã€‚

åœ¨è¿™ä¸‰ä¸ªç±»çš„ä¸‰å±‚ï¼Œæä¾›äº†pipeline()ç”¨äºå¿«é€Ÿå»ºç«‹æ¨¡å‹ï¼Œ`Trainer()`/`TFTrainer()`å¿«é€Ÿè®­ç»ƒæ¨¡å‹ã€‚

**Model classes** æä¾›äº†å„ä¸ªæ¨¡å‹çš„å„ç§æ–¹æ³•ã€‚

**Configuration classes** å­˜å‚¨äº†æ¨¡å‹æ‰€éœ€è¦çš„å‚æ•°



## 5.tokenizer and mask_attened

**input_ids**

åœ¨åŒä¸€ä¸ªtensorè¿›è¡Œtokenizeræ—¶ä¸­ï¼ŒçŸ­åºåˆ—éœ€è¦è¡¥é½ï¼ˆæˆ–è€…é•¿åºåˆ—æˆªæ–­ï¼‰ã€‚

```
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
sequence_a = "This is a short sequence."
sequence_b = "This is a rather long sequence. It is at least longer than the sequence A."
encoded_sequence_a = tokenizer(sequence_a)["input_ids"]
encoded_sequence_b = tokenizer(sequence_b)["input_ids"]
```

```
len(encoded_sequence_a), len(encoded_sequence_b)
ï¼ˆ8ï¼Œ19ï¼‰
padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)
padded_sequences["input_ids"]
è¾“å…¥ï¼š
[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]
```

**attention_mask**è¡¨ç¤ºæ³¨ä¸æ³¨æ„åˆ°è¯¥è¯ï¼Œ1è¡¨ç¤ºæ³¨æ„ï¼Œä¸ç”¨maskæ ‡è®°è¯¥è¯ã€‚è¡¨ç¤ºä¸æ³¨æ„åˆ°è¯¥è¯

```
padded_sequences["attention_mask"]
è¾“å‡ºï¼š
[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
```

**token_type_ids**è¡¨ç¤ºå“ªä¸€æ®µåºåˆ—å¼€å§‹å’Œç»“æŸã€‚

```
encoded_dict['token_type_ids']
è¾“å‡ºï¼š
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

**position_ids**ï¼ˆå¯é€‰å‚æ•°ï¼‰è¡¨ç¤ºtokenæ‰€åœ¨çš„ä½ç½®

**Labels**ï¼ˆå¯é€‰å‚æ•°ï¼‰ç”¨äºåˆ†ç±»è®¡ç®—lossã€‚ç”¨é€”æœ‰ï¼š

â€‹	åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¯¹åº”çš„shapeæ˜¯batch_sizeï¼Œæ¯ä¸ªé¢„æµ‹å€¼å¯¹åº”çœŸå®æ ‡ç­¾ã€‚

â€‹	åœ¨åˆ†è¯åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¯¹åº”çš„shapeæ˜¯batch_size * seq_lengthï¼Œåœ¨æ¯æ¬¡é¢„æµ‹å‡ºçš„token_idå¯¹åº”çœŸå®çš„

â€‹	åœ¨maskè¯­è¨€æ¨¡å‹ä¸­ï¼Œå¯¹åº”çš„shapeæ˜¯batch_size * seq_lengthï¼Œåœ¨æ¯æ¬¡é¢„æµ‹maskå¯¹åº”çš„token_idå’ŒçœŸå®çš„ï¼ˆç®€è€Œè¨€ä¹‹ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹è¯ï¼‰

â€‹	åœ¨åºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ä¸­ï¼Œå¯¹åº”çš„shapeæ˜¯batch_size * tgt_seq_lengthï¼Œæ¯ä¸ªå€¼å¯¹åº”äºä¸æ¯ä¸ªè¾“å…¥åºåˆ—ç›¸å…³è”çš„ç›®æ ‡åºåˆ—

**decoder_input_ids**ï¼šè¿™ä¸ªè¾“å…¥æ˜¯ç‰¹å®šäºç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„ï¼Œå¹¶ä¸”åŒ…å«å°†è¢«è¾“å…¥åˆ°è§£ç å™¨çš„è¾“å…¥idã€‚è¿™äº›è¾“å…¥åº”è¯¥ç”¨äºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼Œä¾‹å¦‚ç¿»è¯‘æˆ–æ‘˜è¦ï¼Œå¹¶ä¸”é€šå¸¸ä»¥ç‰¹å®šäºæ¯ä¸ªæ¨¡å‹çš„æ–¹å¼æ„å»ºã€‚
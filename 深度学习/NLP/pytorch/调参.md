# 调参

[参考地址](https://tsinghua-gongjing.github.io/posts/DL-tricks.html)

## 1. 调参

- **1.1 刚开始，先上小规模数据，模型往大了放**（能用256个filter就别用128个），直接奔着过拟合去（此时都可不用测试集验证集）
  - ​	验证训练脚本的流程。小数据量，速度快，便于测试。
  - 如果小数据、大网络，还不能过拟合，需要检查输入输出、代码是否错误、模型定义是否恰当、应用场景是否正确理解。比较神经网络没法拟合的问题，这种概率太小了。

- **1.2 loss设计要合理**
  - 分类问题：SoftMax，回归：L2 loss。
  - 输出也要做归一化。如果label为10000，输出为0，loss会巨大。
  - 多任务情况时，各个loss限制在同一个量级上。
- **1.3 观察loss胜于观察准确率**
  - 优化目标是loss
  - 准确率是突变的，可能原来一直未0，保持上千代迭代，突变为1
  - loss不会突变，可能之前没有下降太多，之后才稳定学习
- **1.4 确认分类网络学习充分**
  - 分类 =》类别之间的界限
  - 网络从类别模糊到清晰，可以看softmax输出的概率分布。刚开始，可能预测值都在0.5左右（模糊），学习之后才慢慢移动到0、1的极值
- **1.5 学习速率是否设置合理**
  - 太大：loss爆炸或者nan
  - 太小：loss下降太慢
  - 当loss在当前LR下一路下降，但是不再下降了 =》可进一步降低LR
- **1.6 对比训练集和验证集的loss**
  - 可判断是否过拟合
  - 训练是否足够
  - 是否需要early stop
- **1.7 在验证集上调参**

## 2. 模型本身

- 理解网络的原理很重要，CNN的卷积这里，得明白sobel算子的边界检测
- CNN适合训练回答是否的问题
- google的Inception论文，结构要掌握
- 理想的模型：高高瘦瘦的，很深，但是每层的卷积核不多。很深：获得更好的非线性，模型容量指数增加，但是更难训练，面临梯度消失的风险。增加卷积核：可更好的拟合，降低train loss，但是也更容易过拟合。
- 如果训练RNN或者LSTM，务必保证梯度的norm（归一化的梯度）被约束在15或者5
- 限制权重大小：可以限制某些层权重的最大范数以使得模型更加泛化

#### 参数初始化方法

- 用高斯分布初始化
- 用xavier
- word embedding ：xavier训练慢结果差，改为uniform，训练速度飙升，结果也飙升。
- 良好的初始化，可以让参数更接近最优解，这可以大大提高收敛速度，也可以防止落入局部极小。
- relu激活函数：初始化推荐使用He normal
- tanh激活函数：推荐使用xavier（Glorot normal）

------

#### 隐藏层的层数

------

#### 节点数目

------

#### filter

- 用3x3大小
- 数量：2^n
- 第一层的filter数量不要太少，否则根本学不出来（底层特征很重要）

#### 激活函数的选取

- 给神经网络加入一些非线性因素，使得网络可以解决较为复杂的问题
- 输出层：
  - 多分类任务：softmax输出
  - 二分类任务：sigmoid输出
  - 回归任务：线性输出
- 中间层：优先选择relu，有效的解决sigmoid和tanh出现的梯度弥散问题
- CNN：先用ReLU
- RNN：优先选用tanh激活函数

------

#### dropout

- 可防止过拟合，人力成本最低的Ensemble

- 加dropout，加BN，加Data argumentation

- dropout可以随机的失活一些神经元，从而不参与训练

- 例子【

  Dropout 缓解过拟合

  】：

  - 任务：拟合数据点（根据x值预测y值）
  - 构建过拟合网络，比如这里使用了2层，每层节点数=200的网络
  - 使用dropout和不使用dropout，看拟合的效果
  - 可以看到，对于过拟合（模型对训练集拟合得很好）的情况下，使用dropout，能够降低在测试集上的loss，和真实值预测的更贴近。[![20191018141520](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20191018141520.png)](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20191018141520.png)

### 损失函数

------

### 训练相关

------

#### 学习速率

- 在优化算法中更新网络权重的幅度大小
- 可以是恒定的、逐渐下降的、基于动量的或者是自适应的
- **优先调这个LR：会很大程度上影响模型的表现**
- 如果太大，会很震荡，类似于二次抛物线寻找最小值
- 一般学习率从0.1或0.01开始尝试
- 通常取值[0.01, 0.001, 0.0001]
- 学习率一般要随着训练进行衰减。衰减系数设0.1，0.3，0.5均可，衰减时机，可以是**验证集准确率不再上升时**，或**固定训练多少个周期以后自动进行衰减**。
- 有人设计学习率的原则是监测一个比例：每次更新梯度的norm除以当前weight的norm，如果这个值在10e-3附近，且小于这个值，学习会很慢，如果大于这个值，那么学习很不稳定。
- 
  - 红线：初始学习率太大，导致振荡，应减小学习率，并从头开始训练
  - 紫线：后期学习率过大导致无法拟合，应减小学习率，并重新训练后几轮
  - 黄线：初始学习率过小，导致收敛慢，应增大学习率，并从头开始训练

------

#### batch size大小

- 每一次训练神经网络送入模型的样本数
- 可直接设置为16或者64。通常取值为：[16, 32, 64, 128]
- CPU讨厌16，32，64，这样的2的指数倍（为什么？）。GPU不会，GPU推荐取32的倍数。

------

#### momentum大小

- 使用默认的0.9

------

#### 迭代次数

- 整个训练集输入到神经网络进行训练的次数
- 当训练集错误率和测试错误率想相差较小时：迭代次数合适
- 当测试错误率先变小后变大：迭代次数过大，需要减小，否则容易过拟合

------

#### 优化器

- 自适应：Adagrad, Adadelta, RMSprop, Adam
- 整体来讲，Adam是最好的选择
- SGD：虽然能达到极大值，运行时间长，可能被困在鞍点
- Adam: 学习速率3e-4。能快速收敛。

------

#### 残差块和BN

- 残差块：可以让你的网络训练的更深
- BN：加速训练速度，有效防止梯度消失与梯度爆炸，具有防止过拟合的效果
- 构建网络时最好加上这两个组件